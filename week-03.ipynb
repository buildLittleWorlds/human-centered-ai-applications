{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAH 30503 — Week 3: Technical Prototype Strategy\n",
    "\n",
    "**Theme**: From \"there are thousands of AI models\" to \"I know which ones I need and what my app will and won't do.\"\n",
    "\n",
    "---\n",
    "\n",
    "This is the first real code week. You'll run AI pipelines, compare models, read model cards, and scope your MVP. By the end, you'll have a technical plan for what to build next week.\n",
    "\n",
    "And you'll learn the name for the loop you've been practicing for two weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first. It installs everything we need and may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers torch huggingface_hub\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import list_models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Setup complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activity 1: The Model Ecosystem\n",
    "\n",
    "There are 500,000+ free AI models on the [Hugging Face Hub](https://huggingface.co/models). You don't need to browse all of them — you need to find the ones relevant to your project.\n",
    "\n",
    "### Browse Models by Task\n",
    "\n",
    "Run the cell below to see the most popular models for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# See popular models for common task types\n",
    "tasks = [\"text-classification\", \"summarization\", \"question-answering\",\n",
    "         \"ner\", \"translation\", \"text-generation\"]\n",
    "\n",
    "for task in tasks:\n",
    "    models = list(list_models(task=task, sort=\"downloads\", limit=3))\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for m in models:\n",
    "        downloads = f\"{m.downloads:,}\" if m.downloads else \"N/A\"\n",
    "        print(f\"  {m.id}  ({downloads} downloads)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which task type(s) are relevant to your project?** \n",
    "\n",
    "*(Think about your problem from Week 2: what does the AI need to DO? Summarize? Classify? Answer questions? Extract names?)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your First Pipeline: Sentiment Analysis\n",
    "\n",
    "A **pipeline** is like a function that does one AI task. One line of code. Run it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Three test sentences — watch how it handles each one\n",
    "sentences = [\n",
    "    \"I absolutely love this product, it changed my life!\",    # clearly positive\n",
    "    \"This is the worst experience I've ever had.\",             # clearly negative\n",
    "    \"Well, I guess it could have been worse.\",                 # ambiguous\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    result = classifier(s)\n",
    "    label = result[0][\"label\"]\n",
    "    score = round(result[0][\"score\"], 3)\n",
    "    print(f\"  '{s}'\")\n",
    "    print(f\"  → {label} (confidence: {score})\")\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the third result** — the ambiguous sentence. What label did the model give it? What confidence? Do you agree with the model?\n",
    "\n",
    "*(Write your observation here)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pipeline: Summarization\n",
    "\n",
    "Before running the model, **summarize this paragraph yourself in one sentence**:\n",
    "\n",
    "> The invention of the printing press in the 15th century fundamentally transformed European society. Before Gutenberg, books were copied by hand, making them expensive and rare. The printing press made books affordable, which dramatically increased literacy rates across social classes. This democratization of knowledge contributed to the Protestant Reformation, the Scientific Revolution, and eventually the Enlightenment. Scholars could share ideas across borders, building on each other's work in ways that were previously impossible.\n",
    "\n",
    "**Your one-sentence summary**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Now let the model summarize the same paragraph\n",
    "summarizer = pipeline(\"summarization\", device=device)\n",
    "\n",
    "text = (\n",
    "    \"The invention of the printing press in the 15th century fundamentally \"\n",
    "    \"transformed European society. Before Gutenberg, books were copied by hand, \"\n",
    "    \"making them expensive and rare. The printing press made books affordable, \"\n",
    "    \"which dramatically increased literacy rates across social classes. This \"\n",
    "    \"democratization of knowledge contributed to the Protestant Reformation, \"\n",
    "    \"the Scientific Revolution, and eventually the Enlightenment. Scholars could \"\n",
    "    \"share ideas across borders, building on each other's work in ways that were \"\n",
    "    \"previously impossible.\"\n",
    ")\n",
    "\n",
    "result = summarizer(text, max_length=60, min_length=15)\n",
    "print(\"Model's summary:\")\n",
    "print(result[0][\"summary_text\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare your summary to the model's**: What did the model keep? What did it lose? What would you have kept that it dropped?\n",
    "\n",
    "*(Write your comparison here)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Exploration: Try a Pipeline for Your Project\n",
    "\n",
    "Pick a pipeline type relevant to YOUR project and try it on your own input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try a pipeline relevant to your project\n",
    "# Change the task name and input to match your needs\n",
    "#\n",
    "# Available tasks:\n",
    "#   \"sentiment-analysis\"  — positive/negative classification\n",
    "#   \"summarization\"       — condense long text\n",
    "#   \"ner\"                 — extract names, places, organizations\n",
    "#   \"question-answering\"  — answer questions about a passage\n",
    "#   \"text-generation\"     — complete/generate text\n",
    "#   \"translation_en_to_fr\" — translate English to French\n",
    "\n",
    "# Example: Named Entity Recognition\n",
    "my_pipe = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "my_input = \"Barack Obama visited the Eiffel Tower in Paris last summer.\"\n",
    "\n",
    "results = my_pipe(my_input)\n",
    "print(f\"Input: '{my_input}'\")\n",
    "print(\"\\nEntities found:\")\n",
    "for r in results:\n",
    "    print(f\"  {r['word']} → {r['entity_group']} (confidence: {round(r['score'], 3)})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Now try YOUR input — change the task and text below\n",
    "\n",
    "# my_pipe = pipeline(\"YOUR-TASK-HERE\", device=device)\n",
    "# my_input = \"YOUR INPUT TEXT HERE\"\n",
    "# result = my_pipe(my_input)\n",
    "# print(result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activity 2: Model Comparison with Plan Template\n",
    "\n",
    "Now we get structured. Instead of just \"trying stuff,\" you'll **plan a comparison**, execute it, and analyze the results.\n",
    "\n",
    "### Your Comparison Plan\n",
    "\n",
    "Fill this in BEFORE running anything:\n",
    "\n",
    "- **Task**: *(What pipeline type am I investigating?)*\n",
    "\n",
    "- **Question**: *(What am I trying to find out?)*\n",
    "\n",
    "- **Models to compare**: *(List 2-3 models)*\n",
    "\n",
    "- **Inputs**: *(What test inputs will I use? Why these?)*\n",
    "\n",
    "- **Criteria**: *(What am I measuring? Speed? Quality? Relevance to my users?)*\n",
    "\n",
    "- **Expected outcome**: *(What do I THINK I'll find? Write this BEFORE running.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Card Literacy\n",
    "\n",
    "Before comparing models, read the **model cards** for the models you're considering.\n",
    "\n",
    "Go to [huggingface.co/models](https://huggingface.co/models), search for your model, and answer the 5 key questions:\n",
    "\n",
    "A model card is like a nutrition label for AI — it tells you what's inside.\n",
    "\n",
    "#### Model 1: _____________\n",
    "\n",
    "1. **What was it trained on?** *(dataset, domain, language)*\n",
    "\n",
    "2. **What is it intended for?**\n",
    "\n",
    "3. **What are the known limitations?**\n",
    "\n",
    "4. **How was it evaluated?**\n",
    "\n",
    "5. **Who made it and when?**\n",
    "\n",
    "#### Model 2: _____________\n",
    "\n",
    "1. **Trained on?**\n",
    "\n",
    "2. **Intended for?**\n",
    "\n",
    "3. **Limitations?**\n",
    "\n",
    "4. **Evaluated how?**\n",
    "\n",
    "5. **Who/when?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head-to-Head Comparison\n",
    "\n",
    "Run your models on the same inputs and compare. Modify the code below for your task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Head-to-head model comparison\n",
    "# Example: comparing two sentiment models\n",
    "# Change the models and inputs to match YOUR plan template\n",
    "\n",
    "model_a = pipeline(\"sentiment-analysis\",\n",
    "                   model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                   device=device)\n",
    "\n",
    "model_b = pipeline(\"sentiment-analysis\",\n",
    "                   model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "                   device=device)\n",
    "\n",
    "test_inputs = [\n",
    "    \"This restaurant has amazing food and great service!\",\n",
    "    \"The food was okay but nothing special.\",\n",
    "    \"I waited an hour and the order was wrong.\",\n",
    "]\n",
    "\n",
    "print(\"Model A: distilbert (binary: POSITIVE/NEGATIVE)\")\n",
    "print(\"Model B: nlptown (5-star rating)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_inputs:\n",
    "    result_a = model_a(text)[0]\n",
    "    result_b = model_b(text)[0]\n",
    "    print(f\"\\nInput: '{text}'\")\n",
    "    print(f\"  Model A: {result_a['label']} ({round(result_a['score'], 3)})\")\n",
    "    print(f\"  Model B: {result_b['label']} ({round(result_b['score'], 3)})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR comparison — modify for your project's task and models\n",
    "\n",
    "# model_a = pipeline(\"YOUR-TASK\", model=\"MODEL-A-NAME\", device=device)\n",
    "# model_b = pipeline(\"YOUR-TASK\", model=\"MODEL-B-NAME\", device=device)\n",
    "#\n",
    "# test_inputs = [\n",
    "#     \"Your first test input\",\n",
    "#     \"Your second test input\",\n",
    "#     \"Your third test input\",\n",
    "# ]\n",
    "#\n",
    "# for text in test_inputs:\n",
    "#     result_a = model_a(text)\n",
    "#     result_b = model_b(text)\n",
    "#     print(f\"Input: '{text}'\")\n",
    "#     print(f\"  Model A: {result_a}\")\n",
    "#     print(f\"  Model B: {result_b}\")\n",
    "#     print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I Found\n",
    "\n",
    "- **Result**: *(Which model \"won\"? On what criteria?)*\n",
    "\n",
    "- **Surprise**: *(Anything unexpected?)*\n",
    "\n",
    "- **My choice for my project**: *(Model X because...)*\n",
    "\n",
    "- **What the model card told me that the output didn't**: *(from your model card reading)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The PDER Reveal\n",
    "\n",
    "Look at what you just did:\n",
    "\n",
    "1. You had a question → **Plan**\n",
    "2. You ran the models → **Direct**\n",
    "3. You compared results to expectations → **Examine**\n",
    "4. You're about to write down what you learned → **Record**\n",
    "\n",
    "**You've been doing this loop since Week 1.**\n",
    "\n",
    "```\n",
    "    PLAN → DIRECT → EXAMINE → RECORD\n",
    "     ↑                          │\n",
    "     └──────────────────────────┘\n",
    "```\n",
    "\n",
    "It's called **PDER** — Plan, Direct, Examine, Record.\n",
    "\n",
    "- Week 1: You tried AI tools and wrote observations (implicit loop)\n",
    "- Week 2: You planned research questions and compared assumptions to reality (deliberate loop)\n",
    "- Week 3: You wrote a plan template, ran a comparison, and analyzed results (structured loop)\n",
    "\n",
    "The loop doesn't change. What changes is how **sophisticated** each step gets.\n",
    "\n",
    "**Record is the step that makes the whole loop compound.** Every time you write down what you learned, the next cycle starts from a higher baseline. Your CLAUDE.md file IS the Record step, accumulated over 8 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activity 3: Scope the Build\n",
    "\n",
    "### Feature Brainstorm + Impact/Effort Matrix\n",
    "\n",
    "List every feature your app could have. Then map them:\n",
    "\n",
    "```\n",
    "         High Impact\n",
    "              │\n",
    "    ┌─────────┼─────────┐\n",
    "    │  DO      │  PLAN    │\n",
    "    │  FIRST   │  CAREFULLY│\n",
    "    │  (quick  │  (worth   │\n",
    "    │   wins)  │   the     │\n",
    "    │          │   effort) │\n",
    "Low ──────────┼──────────── High\n",
    "Effort        │           Effort\n",
    "    │  MAYBE   │  SKIP     │\n",
    "    │  LATER   │  (tempting│\n",
    "    │          │   but not  │\n",
    "    │          │   worth it)│\n",
    "    └─────────┼─────────┘\n",
    "         Low Impact\n",
    "```\n",
    "\n",
    "**My features mapped to quadrants:**\n",
    "\n",
    "| Feature | Impact | Effort | Quadrant |\n",
    "|---------|--------|--------|----------|\n",
    "| | | | |\n",
    "| | | | |\n",
    "| | | | |\n",
    "| | | | |\n",
    "| | | | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVP Definition\n",
    "\n",
    "The **Minimum Viable Product** is the smallest version someone would actually use more than once.\n",
    "\n",
    "**The Adoption Threshold** (from Week 2 research):\n",
    "For my target users, the threshold is:\n",
    "\n",
    "\n",
    "**MVP Features** (must have — 3 maximum):\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "**Version 2** (after MVP works):\n",
    "1. \n",
    "2. \n",
    "\n",
    "**Explicitly NOT Building** (tempting but cut):\n",
    "1. \n",
    "2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DCS Question: What Knowledge Is Encoded in the Models I Chose?\n",
    "\n",
    "Based on the model cards you read:\n",
    "\n",
    "- **What training data shaped my model?**\n",
    "\n",
    "- **Whose decisions are embedded in it?**\n",
    "\n",
    "- **What domains or perspectives might it underrepresent?**\n",
    "\n",
    "*(Write 2-3 sentences grounded in the specific model card you read, not generic statements about AI)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Record: CLAUDE.md Week 3 Entry\n",
    "\n",
    "Add this to your CLAUDE.md file:\n",
    "\n",
    "```\n",
    "## Week 3: Technical Prototype Strategy\n",
    "\n",
    "### PDER Named\n",
    "The loop I've been practicing: Plan → Direct → Examine → Record.\n",
    "Plan = decide what to investigate. Direct = run it.\n",
    "Examine = compare results to expectations. Record = write it down.\n",
    "\n",
    "### Pipeline Pattern\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"task-name\")\n",
    "result = pipe(\"my input\")\n",
    "Pipelines I explored: [list]\n",
    "Most relevant to my project: [which and why]\n",
    "\n",
    "### Model Selection\n",
    "For [task], I compared [models]. My choice: [X] because [Y].\n",
    "Model card: trained on [data], intended for [use], limited by [limitation].\n",
    "\n",
    "### MVP Spec\n",
    "MVP features: [3 must-haves]\n",
    "Explicitly not building: [what was cut and why]\n",
    "Adoption threshold: [from Week 2, refined]\n",
    "\n",
    "### DCS: What Knowledge Is Encoded?\n",
    "[What training data, whose decisions, what's underrepresented]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next\n",
    "\n",
    "You have a plan, a model, and a scope. You know what you're building, what you're skipping, and what AI tools you'll use.\n",
    "\n",
    "**Next week**: You build it. You'll direct Claude Code to create your application, wrap it in a Gradio interface (a real app with a UI), and watch someone else use it. You'll also learn the 6-question examination protocol — a structured way to evaluate what you built."
   ]
  }
 ]
}